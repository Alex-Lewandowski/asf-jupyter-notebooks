{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.png\" width=\"100%\" />\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"7\"> <b> GEOS 657: Microwave Remote Sensing<b> </font>\n",
    "\n",
    "<font size=\"5\"> <b>Lab 9: InSAR Time Series Analysis using GIAnT within Jupyter Notebooks<br>Part 1: Data Download & Preprocessing from a SARVIEWS Import <font color='rgba(200,0,0,0.2)'> -- [## Points] </font> </b> </font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer, Joshua J C Knicely, Alex Lewandowski, Rowan Biessel; University of Alaska Fairbanks</b> <br>\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" /><font color='rgba(200,0,0,0.2)'> <b>Due Date: </b>NONE</font>\n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> This Lab is part of the UAF course <a href=\"https://radar.community.uaf.edu/\" target=\"_blank\">GEOS 657: Microwave Remote Sensing</a>. This lab is divided into 3 parts: 1) data download and preprocessing, 2) GIAnT time series, and 3) a simple Mogi source inversion. The primary goal of this lab is to demonstrate how to download the requisite data, specifically interferograms, and preprocess them for use with the Generic InSAR Analysis Toolbox (<a href=\"http://earthdef.caltech.edu/projects/giant/wiki\" target=\"_blank\">GIAnT</a>) in the framework of *Jupyter Notebooks*.<br>\n",
    "\n",
    "<b>Our specific objectives for this lab are to:</b>\n",
    "\n",
    "- Download data using ASF tools: \n",
    "    - From a SARVIEWS subscription. \n",
    "- Pre-process data: \n",
    "    - Subset (crop) the data to an Area of Interest (AOI). \n",
    "    - Verify the quality of the data.\n",
    "    - Cull data selection based on a timeframe and orbital characteristics. \n",
    "    - Reproject interferograms to a uniform UTM zone. \n",
    "    - Use TRAIN to remove static atmospheric effects related to surface elevation.\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> Target Description </b> </font>\n",
    "\n",
    "<font size=\"3\"> In this lab, we will download interferograms covering a SARVIEWS area of interest.  </font>\n",
    "\n",
    "<font size=\"4\"> <font color='rgba(200,0,0,0.2)'> <b>THIS NOTEBOOK INCLUDES NO HOMEWORK ASSIGNMENTS.</b></font> <br>\n",
    "\n",
    "Contact me at fjmeyer@alaska.edu should you run into any problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>Overview</b></font>\n",
    "<br>\n",
    "<font size='3'><b>About TRAIN</b>\n",
    "<br>\n",
    "The tropospheric correction is one of the most significant challanges in InSAR. Without this correction, surface deformation signals can go completely unnoticed, or, perhaps worse, a false signal caused by the atmosphere can be taken as an accurate representation of surface deformation. This can often occur with volcanoes due to the characteristics of the atmosphere as well as the surface elevation (i.e., a taller point on the volcano means the InSAR signal passed through less atmosphere and will be affected differently from a point lower on the volcano). <br>We will use the Toolbox for Reducing Atmospheric InSAR Noise (TRAIN) today. The purpose of TRAIN is to add state of the art tropospheric correction methods to the InSAR processing chain. It can include corrections that are phase-based, using spectrometers, using weather models, and even data from balloon soundings. \n",
    "<br><br>\n",
    "<b>Limitations</b>\n",
    "<br>\n",
    "The particular version we are using was created by the Alaska Satellite Facility. ASF took the original MATLAB code developed by David Bekaert created a Python2.7 wrapper (and more recently, a Python3.7 wrapper)for it. Currently, it only allows processing using MERRA2 data. Including other data types can be done relatively easily, though it does require modification of the existing python code. \n",
    "<br>\n",
    "Each of the correction methods included in TRAIN is ideal for different locations and conditions. Spectrometers provide the best correction, but can only be used in cloud-free and daylight conditions. Phase-based and weather model correction methods capture regional signals well, but fail to capture turbulent tropospheric signals. \n",
    "<br><br>\n",
    "More information about TRAIN, its capabilities, and its limitations can be found in <a href=\"http://www.sciencedirect.com/science/article/pii/S0034425715301231\">Bekaert et al. [2015]</a>, at David Dekaert's <a href=\"http://davidbekaert.com/#links\">webpage</a>, or at the <a href=\"https://github.com/asfadmin/hyp3-TRAIN\" target=\"_blank\">ASF Github</a>. \n",
    "<br><br>\n",
    "<b>Steps to use TRAIN</b><br>\n",
    "\n",
    "- System Setup\n",
    "    - Import Python Packages\n",
    "    - Set User Inputs\n",
    "- Download and Preprocess Data\n",
    "    - Access SARVIEWs Subscriptions\n",
    "    - Download and unzip the Data\n",
    "    - Project all Geotiffs to the Same UTM Zone\n",
    "    - Mosaic Geotiffs with Partial Coverage\n",
    "- Identify Area of Interest\n",
    "- Subset (Crop) Data to Area of Interest\n",
    "    - Subset the Data\n",
    "    - Check that Subsetted Geotiffs have Pixels\n",
    "    - Check the Dimensions of Subsetted Geotiffs\n",
    "- Create Input Files and Code for TRAIN\n",
    "    - Create <font face='Courier New'>parms_aps.txt</font> file\n",
    "    - Create <font face='Courier New'>ifgday.mat</font> file\n",
    "    - Convert Subsetted Tiffs to GCS Coordinates\n",
    "    - Adjust file names\n",
    "- Run TRAIN\n",
    "    - Minor Set Up\n",
    "    - Steps 0-3\n",
    "    - Step 4\n",
    "    - Comparison of Corrected and Uncorrected Unwrapped Phase\n",
    "    - Convert back to the original coordinate system\n",
    "\n",
    "<br><br>\n",
    "When you use TRAIN, cite the creator's work using:<br>\n",
    "Bekaert et al., RSE 2015, \"Statistical comparison of InSAR tropospheric correction techniques.\" <br>&emsp;Open access: http://www.sciencedirect.com/science/article/pii/S0034425715301231\n",
    "<!-- <br><br><b><i>DO WE NEED TO ALSO GIVE ASF A CITATION???</i></b> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>0. Setup</b></font><br>\n",
    "    <font size='3'>We will first import requisite Python libraries and modules, create or define an analysis directory, and define all of our user inputs. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size=\"4\"> <b> 0.0 Import Python Packages and Enable Extensions</b></font>    <br>\n",
    "    <font size='3'>First, we will import all needed libraries and modules. </font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import pathlib\n",
    "from datetime import datetime, date\n",
    "from copy import copy\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "import gdal\n",
    "import pyproj\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from matplotlib import animation\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Markdown\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "#from asf_notebook import path_exists\n",
    "from asf_notebook import remove_nan_filled_tifs\n",
    "from asf_notebook import select_parameter\n",
    "from asf_notebook import EarthdataLogin\n",
    "from asf_notebook import get_wget_cmd\n",
    "from asf_notebook import asf_unzip\n",
    "from asf_notebook import get_hyp3_subscriptions\n",
    "from asf_notebook import gui_date_picker\n",
    "#from asf_notebook import get_product_info\n",
    "from asf_notebook import get_subscription_products_info\n",
    "from asf_notebook import date_from_product_name\n",
    "from asf_notebook import get_products_dates_insar\n",
    "from asf_notebook import get_slider_vals\n",
    "from asf_notebook import input_path\n",
    "from asf_notebook import handle_old_data\n",
    "from asf_notebook import AOI_Selector\n",
    "from asf_notebook import jupytertheme_matplotlib_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"4\"> <b> 0.1 Define an Analysis Directory</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    sub_dir = input_path(\n",
    "        f\"\\nPlease enter the name of a directory in which to store your data for this analysis.\")\n",
    "    if os.path.exists(sub_dir):\n",
    "        contents = glob.glob(f'{sub_dir}/*')\n",
    "        if len(contents) > 0:\n",
    "            choice = handle_old_data(sub_dir, contents)\n",
    "            if choice == 1:\n",
    "                shutil.rmtree(sub_dir)\n",
    "                os.mkdir(sub_dir)\n",
    "                break\n",
    "            if choice == 2:\n",
    "                break\n",
    "            else:\n",
    "                clear_output()\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        os.mkdir(sub_dir)\n",
    "        break\n",
    "os.chdir(sub_dir)\n",
    "home = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"4\"> <b> 0.2 Set Paths and TRAIN Parameters </b></font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>0.2.0 Create directories we will need later in the notebook</b><br></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictonary to hold values we wish to pickle and use in the Part 2 notebook\n",
    "to_pickle = dict()\n",
    "\n",
    "# Create the folder that will hold the full interferograms and their associated files. \n",
    "ingram_folder = 'ingrams'\n",
    "pathlib.Path(ingram_folder).mkdir(parents=True, exist_ok=True)\n",
    "to_pickle.update({'ingram_folder': ingram_folder})\n",
    "\n",
    "# Create the folder in which we wish to store our interferogram subsets. \n",
    "subset_folder = 'ingram_subsets'\n",
    "pathlib.Path(subset_folder).mkdir(parents=True, exist_ok=True)\n",
    "to_pickle.update({'subset_folder': subset_folder})\n",
    "\n",
    "            \n",
    "# Create the folder in which we wish to store our converted interferogram subsets. \n",
    "# This is important later in the notebook when we convert our subsets from a \n",
    "# local geographic coordinate system to decimal degrees. \n",
    "corrected_folder = 'ingram_subsets_converted'\n",
    "pathlib.Path(corrected_folder).mkdir(parents=True, exist_ok=True)\n",
    "to_pickle.update({'corrected_folder': corrected_folder})\n",
    "\n",
    "\n",
    "train_dir = '' # temporary blank directory\n",
    "merra2_datapath = './MERRA2'\n",
    "dem_path = os.path.join(train_dir,'myDEM.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>0.2.1 Designate TRAIN Input Parameters</b><br></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_data_type = 'ECMWF'\n",
    "ifgday_file = 'ifgday.mat'\n",
    "lambda_meters = 0.055465763 \n",
    "incidence_angle = 38.5/180*np.pi # This needs to be in radians. \n",
    "extra = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>0.3 Earthdata Login</b><br></font>\n",
    "<font size='3'>To download data from ASF, we need to provide our <a href=\"https://www.asf.alaska.edu/get-data/get-started/free-earthdata-account/\" target=\"_blank\">NASA Earth Data</a> username to the system. Setup an EarthData account if you do not yet have one. <font color='rgba(200,0,0,0.2)'><b>Note that EarthData's End User License Agreement (EULA) applies when accessing the Hyp3 API from this notebook. If you have not acknowleged the EULA in EarthData, you will need to navigate to <a href=\"https://earthdata.nasa.gov/\" target=\"_blank\">EarthData's home page</a> and complete that process.</b></font><br><br>\n",
    "    For some data processing later, we will also need to add <b>NASA GESDISC DATA ARCHIVE</b> to our list of approved applications. This is needed to provide access to MERRA2 data files needed by TRAIN to perfrom the atmospheric correction. This can be done by going to your <a href=\"https://urs.earthdata.nasa.gov/profile\" target=\"_blank\">EarthData's profile page</a>, clicking <b>Applications</b> and selecting <b>Approved Applications</b> from the drop down menu, select <b>Approve More Applications</b> at the bottom left, search for <b>NASA GESDISC DATA ARCHIVE</b>, select it, and agree to the terms and conditions. Once that is complete, you will have access to the MERRA2 data and TRAIN will be able to automatically download whichever files it requires. \n",
    "<br><br>\n",
    "<b>Login to Earthdata:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "login = EarthdataLogin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font face=\"Calibri\">\n",
    "\n",
    "<font size=\"5\"> <b> 1. Download and Preprocess Data</b> </font>\n",
    "\n",
    "<font size=\"3\"> We will now access our subscriptions for download. \n",
    "    <br>\n",
    "    1.0 and 1.1 demonstrate downloading from non-SARVIEWS and SARVIEWS subscriptions. \n",
    "<br><br>\n",
    "<font size='4'><b>1.0 Access Non-SARVIEWS HyP3 Subscriptions</b><br></font>\n",
    "Skip ahead to section 1.1 for SARVIEWS\n",
    "<br><br>\n",
    "<font size=\"3\"><b>1.0.0 Enter a Group ID if downloading from a non-SARVIEWS group subscription:</b> </font>\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    group_id = input(\"Leave blank or enter a Group ID\")\n",
    "    if group_id == '':\n",
    "        group_id = None\n",
    "        break\n",
    "    try:\n",
    "        int(group_id)\n",
    "    except ValueError:\n",
    "        print(\"ValueError: Enter an integer.\")\n",
    "        pass\n",
    "    else:\n",
    "        clear_output()\n",
    "        print(f\"Group ID: {group_id}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>1.0.1 Select a subscription</b> </font>\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriptions = get_hyp3_subscriptions(login, group_id=group_id)\n",
    "subscription_id = None\n",
    "if len(subscriptions) > 0:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a subscription, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Subscription:')\n",
    "    subscription_id = select_parameter(subscriptions, '')\n",
    "subscription_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size='3'><b>1.0.2 Save subscription choice</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    subscription_id = subscription_id.value.split(':')[0]\n",
    "    print(subscription_id)\n",
    "except AttributeError:\n",
    "    pass\n",
    "    print(\"AttributeError: 'str' object has no attribute 'value'\")\n",
    "    display(Markdown(\"<text style='color:darkred;'>Try re-running the previous code cell.</text>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size='3'><b>1.0.3 Load a date selection widget containing the subscription's date range</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"<text style='color:darkred;'>Note: After selecting a date range, you should select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "display(Markdown(\"<text style='color:darkred;'>Otherwise, you may simply rerun this code cell.</text>\"))\n",
    "print('\\nSelect a Date Range:')\n",
    "products_info = get_subscription_products_info(subscription_id, login, group_id=group_id)\n",
    "dates = get_products_dates_insar(products_info)\n",
    "date_picker = gui_date_picker(dates)\n",
    "date_picker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size='3'><b>1.0.4 Save the selected date range.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = get_slider_vals(date_picker)\n",
    "date_range[0] = date_range[0].date()\n",
    "date_range[1] = date_range[1].date()\n",
    "print(f\"Date Range: {str(date_range[0])} to {str(date_range[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size='3'><b>1.0.5 Create a list of download URLs for all products within our date range.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_urls = []\n",
    "for p in products_info:\n",
    "    dt = date_from_product_name(p['granule']).split('T')[0]\n",
    "    dt = datetime.strptime(dt, '%Y%m%d').date()\n",
    "    if dt >= date_range[0] and dt <= date_range[1]:\n",
    "        download_urls.append(p['url'])\n",
    "download_urls.sort()\n",
    "print(f\"There are {len(download_urls)} products to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>1.1 Access SARVIEWs Subscription</b><br></font>\n",
    "<font size='3'>The below code is a demonstration of how to download data via a SARVIEWs subscription. The selected HyP3 product IDs are stored in the URL. <br>The first cell below acquires user credentials and creates a .netrc file in order to access Earthdata. The 2nd cell executes some javascript that loads this URL into the notebook's python kernel. Subsequently, this URL is parsed and the IDs are extracted. Using the supplied event ID, the following cell compares each of the IDs to all of the products in this event group and then returns a list of URLS from products whos id is included in the URL.  <font face='Courier New'>products_temp</font>. The user name and password are the corresponding Earthdata user login. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>1.1.0 Use Javascript to extract this notebook's URL </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%javascript \n",
    "var kernel = Jupyter.notebook.kernel; \n",
    "var command = [\"notebookUrl = \",\n",
    "               \"'\", window.location, \"'\" ].join('')\n",
    "// alert(command)\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>1.1.1 Parse the URL and retrieve product IDs and Event</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_url = urlparse(notebookUrl)\n",
    "params = parse_qs(parsed_url.query)\n",
    "try:\n",
    "    ids = params['ids'][0].split(',')\n",
    "    event_id = params['event'][0]\n",
    "    \n",
    "    if len(ids) == 0:\n",
    "        print('No products were found from the url')\n",
    "    else:\n",
    "        print(f'Found {len(ids)} product IDs in the URL to prepare: ')\n",
    "        for product in ids:\n",
    "            print(product)\n",
    "\n",
    "    print(f'Using eventId: {event_id}')    \n",
    "except:\n",
    "    display(Markdown(f'<text style=color:darkred> ERROR: Missing Data</text>'))\n",
    "    display(Markdown(f'<text style=color:darkred> Go to <a href=\"http://sarviews-test.s3-website-us-east-1.amazonaws.com/\" target=\"_blank\"> sarviews-hazards.alaska.edu </a> to find SARVIEWS data.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>1.1.2 Use the Hyp3 API to fetch product download URLs</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get download URLs using the event ID \n",
    "groups = login.api.get_groups_public(id=event_id)\n",
    "groupName = groups[0]['name']\n",
    "print(f'Loading Products from {groupName}...')\n",
    "\n",
    "# Load every page of products\n",
    "event_prods = []\n",
    "page = 1\n",
    "emptyQuery = False\n",
    "while not emptyQuery:\n",
    "    new_prods =  login.api.get_products_public(group_id=event_id, page=(page - 1))\n",
    "    event_prods += new_prods\n",
    "    if len(new_prods) == 0:\n",
    "        emptyQuery = True\n",
    "    else:\n",
    "        print(f'Loaded page {page} of products')\n",
    "        page += 1\n",
    "        \n",
    "download_urls = []\n",
    "for product in event_prods:\n",
    "    if str(product['id']) in ids:\n",
    "        download_urls.append(product['url'])\n",
    "        \n",
    "if (len(download_urls) == len(ids)):\n",
    "    print('All IDs are accounted for with download URLS! Ready to download.')\n",
    "else: \n",
    "    print(f'Only {len(download_urls)} products match those selected from the URL and are ready to download. ')\n",
    "download_urls.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b>1.3 Download and unzip the data. </b></font>\n",
    "<br>\n",
    "<font size='3' face='Calibri'>We should now have a list of the products we wish to download. The code below will download the requisite zip files, unpack them into our designated folder, and then delete any remaining zip files. Removing the zip files helps to reduce space usage.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'><b>1.3.0 Download the Products</b><br></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(download_urls)} products to download.\")\n",
    "full_ingram_path = f\"{home}/{ingram_folder}\"\n",
    "if os.path.exists(full_ingram_path):\n",
    "    product_count = 1\n",
    "    for url in download_urls:\n",
    "        print(f\"\\nProduct Number {product_count} of {len(download_urls)}:\")\n",
    "        product_count += 1\n",
    "        \n",
    "        parsed = urlparse(url)\n",
    "        file_name = os.path.basename(parsed.path) \n",
    "        # print(f'Filename: {file_name}')\n",
    "        \n",
    "        # if not already present, we need to download and unzip products\n",
    "        product_folder = file_name.split('.zip')[0]\n",
    "        if not os.path.exists(product_folder):\n",
    "            print(\n",
    "                f\"\\n{product_folder} is not present.\\nDownloading from {url}\")\n",
    "            cmd = get_wget_cmd(url, login)\n",
    "            !$cmd\n",
    "            if os.path.exists(file_name):\n",
    "                zippedProduct = f\"{home}/{file_name}\"\n",
    "                zippedProduct = zippedProduct.split('\\n')[0]\n",
    "                print(f\"zipped Product: {zippedProduct}\")\n",
    "\n",
    "                asf_unzip(full_ingram_path, zippedProduct)\n",
    "\n",
    "                try:\n",
    "                    os.remove(file_name)\n",
    "                except OSError:\n",
    "                    pass\n",
    "                print(f\"\\nDone.\")\n",
    "            else:\n",
    "                print('Download failed, does this HyP3 product still exist?')\n",
    "        else:\n",
    "            print(f\"{product_folder} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\">\n",
    "    <font size=\"5\"> <b> 2. Prepare the Tiffs for Processing</b> </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>2.0 Gather Amplitude, Coherence, and Interferogram Paths</b></font><br><br>\n",
    "    <font size='3'><b>2.0.0 Write functions to grab and print the path information for the amplitude, unwrapped phase, and the coherence files.</b></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_paths(paths):\n",
    "    tiff_paths = !ls $paths | sort -t_ -k5,5\n",
    "    return tiff_paths\n",
    "\n",
    "def print_tiff_paths(tiff_paths):\n",
    "    print(\"Tiff paths:\")\n",
    "    for p in tiff_paths:\n",
    "        print(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.0.1 Call the functions we just wrote to gather the file path information.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_amp    = f\"{ingram_folder}/*/*_amp.tif\"\n",
    "paths_ingram = f\"{ingram_folder}/*/*_unw_phase.tif\"\n",
    "paths_cohr   = f\"{ingram_folder}/*/*_corr.tif\"\n",
    "amp_paths    = get_tiff_paths(paths_amp)\n",
    "ingram_paths = get_tiff_paths(paths_ingram)\n",
    "cohr_paths    = get_tiff_paths(paths_cohr)\n",
    "print(f\"amp_path[0]: {amp_paths[0]}\")\n",
    "print(f\"ingram_path[0]: {ingram_paths[0]}\")\n",
    "print(f\"cohr_path[0]: {cohr_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.1 Orbit Direction</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>We will acquire orbit information about our interferograms, retain only those that match our desired orbit direction, and then pickle the variable <font face='Courier New'>heading_avg</font> for use later in Part 2 of this lab. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.1.0 Write functions to filter product paths by orbit direction and file type</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interferogram_headings(metadata_paths: list) -> dict:\n",
    "    headings = {}\n",
    "    for path in metadata_paths:\n",
    "        with open(path, 'r') as metadata:\n",
    "            for line in metadata:\n",
    "                t = line.split(':')\n",
    "                if 'Heading' in t[0]:\n",
    "                    headings.update({parse_interferogram_name(path): float(t[1])})\n",
    "    return headings\n",
    "\n",
    "def get_predominate_orbit_direction(headings: list) -> str:\n",
    "    ascending = 0\n",
    "    descending = 0\n",
    "    for heading in headings:\n",
    "        if abs(heading) < 90.0:\n",
    "            ascending += 1\n",
    "        else:\n",
    "            descending += 1\n",
    "    if ascending >= descending:\n",
    "        return 'ascending'\n",
    "    else:\n",
    "        return 'descending'    \n",
    "\n",
    "def parse_interferogram_name(path: str) -> str:\n",
    "    regex = '[0-9]{8}T[0-9]{6}_[0-9]{8}T[0-9]{6}'\n",
    "    return re.search(regex, path)[0]\n",
    "\n",
    "def filter_by_orbit(paths, orbit, headings):\n",
    "    pths = copy(paths)\n",
    "    for path in paths:\n",
    "        if orbit == 'ascending' and \\\n",
    "        abs(headings[parse_interferogram_name(path)]) >= 90.0:\n",
    "            pths.remove(path)\n",
    "        elif orbit == 'descending' and \\\n",
    "        abs(headings[parse_interferogram_name(path)]) < 90.0:\n",
    "            pths.remove(path)\n",
    "    return pths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>2.1.1 Use heading directions from the product metadata to find the predominate orbit direction. \n",
    "    <br>\n",
    "Filter the path lists to the data by the predominate orbit direction.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of paths to interferogram metadata\n",
    "metadata_paths = []\n",
    "for path in amp_paths:\n",
    "    metadata_paths.append(path.replace('_amp.tif', '.txt'))\n",
    "    \n",
    "# Create a dictionary of interferogram headings \n",
    "# and find the average heading\n",
    "headings = get_interferogram_headings(metadata_paths)\n",
    "heading_avg = np.mean(list(headings.values()))\n",
    "print(f'Average Heading: {heading_avg}')\n",
    "\n",
    "# Determine the predominate orbit direction\n",
    "orbit = get_predominate_orbit_direction(list(headings.values())) \n",
    "print(f'Primary orbit: {orbit}')\n",
    "\n",
    "# Gather lists of amp, ingram, and cohrs paths\n",
    "amps = filter_by_orbit(amp_paths, orbit, headings) \n",
    "ingrams = filter_by_orbit(ingram_paths, orbit, headings)\n",
    "cohrs = filter_by_orbit(cohr_paths, orbit, headings)\n",
    "\n",
    "print(len(amps)) # if this is zero, then something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.1.2 Pickle <font face='Courier New'>heading_avg</font> for GIAnT</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>We will need the variable average heading in the Part 2 notebook for this lab, when using GIAnT.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle.update({'heading_avg': heading_avg})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.2 Project all data to the Same UTM Zone</b><br></font>\n",
    "<font size='3'><font face='Calibri'><font size='3'>Some of the geotiffs may use different UTM zones. In the code below, we will identify the predominate UTM zone and reproject the rest into that zone. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3' face='Calibri'><b>2.2.0: Create a list of all of the geotiff files.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = amps + ingrams + cohrs\n",
    "# print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.2.1 Gather the UTM definition types and zones for each product and determine which is the most commonly used in the data stack</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utm_zones_types(tiff_paths):\n",
    "    utm_zones = []\n",
    "    utm_types = []\n",
    "    print('Checking UTM Zones in the data stack ...\\n')\n",
    "    for i, path in enumerate(tiff_paths):\n",
    "        info = (gdal.Info(path, options = ['-json']))\n",
    "        info = (json.loads(info))['coordinateSystem']['wkt']\n",
    "        zone = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "        utm_zones.append(zone)\n",
    "        typ = info.split('ID')[-1].split('\"')[1]\n",
    "        utm_types.append(typ)\n",
    "    return utm_zones, utm_types\n",
    "\n",
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "print(f\"Unique UTM Zones: {list(set(utm_zones))}\")\n",
    "print(f\"Unique UTM Types: {list(set(utm_types))}\\n\")\n",
    "\n",
    "utm_unique, counts = np.unique(utm_zones, return_counts=True)\n",
    "a = np.where(counts == np.max(counts))\n",
    "predominate_utm = utm_unique[a][0]\n",
    "print(f\"Predominate UTM Zone: {predominate_utm}\")\n",
    "print(f\"Number of UTM Zones:  {len(utm_unique)}\")\n",
    "to_pickle.update({'utm': predominate_utm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.2.2 Make a list of indicies in utm_zones that need to be reprojected.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != predominate_utm]\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print(reproject_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.2.3 Call gdalwarp to reproject any geotiffs not in the predominant UTM zone.</b> These will be stored in a new file with a leading 'r' for identification. The originals are then deleted and the new geotiffs renamed. These new geotiffs are placed in an entirely new file as GDAL may overwrite parts of the original file before accessing them. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject_indicies = [i for i, j in enumerate(utm_zones) if j != predominate_utm]\n",
    "print('--------------------------------------------')\n",
    "print('Reprojecting %4.1f files' %(len(reproject_indicies)))\n",
    "print('--------------------------------------------')\n",
    "for k in reproject_indicies:\n",
    "    temppath = tiff_paths[k].strip()\n",
    "    _, product_name, tiff_name = temppath.split('/')\n",
    "    cmd = f\"gdalwarp -overwrite ingrams/{product_name}/{tiff_name}\"\\\n",
    "          f\" ingrams/{product_name}/r{tiff_name} -s_srs {utm_types[k]}:\"\\\n",
    "          f\"{utm_zones[k]} -t_srs EPSG:{predominate_utm}\"\n",
    "    print(cmd)\n",
    "    !$cmd\n",
    "    rm_cmd = f\"rm {temppath}\"\n",
    "    !$rm_cmd\n",
    "    os.rename(f\"ingrams/{product_name}/r{tiff_name}\", temppath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.2.4 Double check that all of the 'tiff's now have the same UTM Zone and type and assign it to the 'utm' variable..</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "zones = list(set(utm_zones))\n",
    "print(f\"Unique UTM Zones: {zones}\")\n",
    "print(f\"Unique UTM Types: {list(set(utm_types))}\")\n",
    "\n",
    "if len(zones) == 1:\n",
    "    utm = utm_zones[0][:]\n",
    "    print(f\"UTM Zone: {utm}\")\n",
    "else:\n",
    "    print(\"After reprojection, there should be a single utm zone.\")\n",
    "    print(f\"There are currently {len(zones)} zones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>2.3 Mosaic Geotiffs with Partial Coverage</b><br></font>\n",
    "    <font size='3'>Merge multiple frames of the same type and from the same date into a single geotiff. This code makes the assumption that any imagery taken on the same day are frames that do not overlap the same areas. For Sentinel1 imagery, this holds true; for other data sources, it may not. </font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.3.0 Get the paths for the files to be merged.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_full_amp  = f\"{ingram_folder}/*/*_amp.tif\"\n",
    "paths_full_corr = f\"{ingram_folder}/*/*_corr.tif\"\n",
    "paths_full_unw  = f\"{ingram_folder}/*/*_unw_phase.tif\"\n",
    "amp_full_paths  = get_tiff_paths(paths_full_amp)\n",
    "corr_full_paths = get_tiff_paths(paths_full_corr)\n",
    "unw_full_paths  = get_tiff_paths(paths_full_unw)\n",
    "print_tiff_paths(amp_full_paths+corr_full_paths+unw_full_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>2.3.1 Gather date info from filenames</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(paths):\n",
    "    dates = []\n",
    "    for p in paths:\n",
    "        name = parse_interferogram_name(p)\n",
    "        date1 = name.split(\"_\")[0][0:8]\n",
    "        date2 = name.split(\"_\")[1][0:8]\n",
    "        dates.append([date1,date2])\n",
    "    dates.sort()\n",
    "    return dates       \n",
    "\n",
    "dates = get_dates(unw_full_paths)\n",
    "for datepair in dates: print(datepair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.3.2 Create a list of groups of paths to products which share acquisition dates</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_date_batches = []\n",
    "dup_dates = []\n",
    "for i in range(0,len(dates)-1):\n",
    "    dte = dates[i]\n",
    "    for j in range(i+1,len(dates)):\n",
    "        if dates[j] == dte:\n",
    "            dup_dates.append(dte)\n",
    "print(f\"dup_dates: {dup_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.3.3 Write a function using gdal_merge to merge products with duplicate dates</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dup_pairs(dups, paths):\n",
    "    for date_pair in dups:\n",
    "        matching = [s for s in paths if all(xs in s for xs in date_pair)]\n",
    "        if len(matching) == 2:\n",
    "            # create the output file name and merge the two files\n",
    "            # with matching sets of dates. \n",
    "            path,file = os.path.split(matching[0])\n",
    "            outputFile = os.path.join(path, 'MERGED'+file)\n",
    "            cmd = f\"gdal_merge.py -o {outputFile} {matching[0]} {matching[1]}\"\n",
    "            !{cmd}\n",
    "            \n",
    "            # The below code does some clean up. \n",
    "            # First, delete the unmerged original\n",
    "            os.remove(matching[0])\n",
    "            # Second, rename merged file to be that of the original file\n",
    "            os.rename(outputFile,matching[0])\n",
    "            # Third, remove matching[1] from the list of paths. \n",
    "            paths.remove(matching[1])\n",
    "        else:\n",
    "            print(f\"Error: there is not a pair of matching entries.\")\n",
    "            print(f\"Number of matches: {len(matching)}\")\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>2.3.4 Call merge_dup_pairs to merge amp, unw, and corr products with duplicated dates</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_dup_pairs(dup_dates, amp_full_paths)\n",
    "merge_dup_pairs(dup_dates, unw_full_paths)\n",
    "merge_dup_pairs(dup_dates, corr_full_paths)\n",
    "tiff_paths = amp_full_paths + unw_full_paths + corr_full_paths\n",
    "print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='5'> <b> 3. Subset Area of Interest</b> </font>\n",
    "    <br>\n",
    "    <font size='3'> Here we identify our area of interest (AOI). Our AOI must contain all of the expected deformation and a surrounding region of little to no deformation. Following our selection of this region, we will subset our data to this region. This helps reduce computation time. </font>\n",
    "    <br><br>\n",
    "    <font size='4'><b>3.0 Identify AOI</b></font> <br><br>\n",
    "    <font size='3'><b>3.0.0 Update the paths to the amplitude data after merging in the previous section</b></font> \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amp_tiff_paths = get_tiff_paths(paths_full_amp)\n",
    "# print_tiff_paths(amp_tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size=\"3\"> <b>3.0.1 Merge together one image from each area represented in the stack for display in the AOI selector:</b> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = {}\n",
    "for pth in amp_tiff_paths:\n",
    "    info = (gdal.Info(pth, options = ['-json']))\n",
    "    info = (json.loads(info))['wgs84Extent']['coordinates']\n",
    "    \n",
    "    coords = [info[0][0], info[0][3]]\n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 2):\n",
    "            coords[i][j] = round(coords[i][j])\n",
    "    str_coords = f\"{str(coords[0])}{str(coords[1])}\"\n",
    "    if str_coords not in to_merge:\n",
    "        to_merge.update({str_coords: pth})\n",
    "merge_paths = \"\"\n",
    "for pth in to_merge:\n",
    "    merge_paths = f\"{merge_paths} {to_merge[pth]}\"  \n",
    "print(merge_paths)\n",
    "\n",
    "full_scene = f\"{home}/full_scene.tif\"\n",
    "if os.path.exists(full_scene):\n",
    "    os.remove(full_scene)\n",
    "gdal_command = f\"gdal_merge.py -o {full_scene} {merge_paths}\"\n",
    "!{gdal_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.2 Create a VRT (virtual raster stack) from merged images and convert that into array:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = f\"{home}/raster_stack.vrt\"\n",
    "!gdalbuildvrt -separate $image_file -overwrite $full_scene\n",
    "\n",
    "img = gdal.Open(image_file)\n",
    "rasterstack = img.ReadAsArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.3 Create an AOI selector from your raster stack:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig_xsize = 7.5\n",
    "fig_ysize = 7.5\n",
    "aoi = AOI_Selector(rasterstack, fig_xsize, fig_ysize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.4 Gather and define projection details:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotrans = img.GetGeoTransform()\n",
    "projlatlon = pyproj.Proj('EPSG:4326') # WGS84\n",
    "projimg = pyproj.Proj(f'EPSG:{utm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.5 Write a function to convert the pixel, line coordinates from the AOI selector into geographic coordinates in the stack's EPSG projection:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geolocation(x, y, geotrans,latlon=True):\n",
    "    ref_x = geotrans[0]+x*geotrans[1]\n",
    "    ref_y = geotrans[3]+y*geotrans[5]\n",
    "    if latlon:\n",
    "        ref_y, ref_x = pyproj.transform(projimg, projlatlon, ref_x, ref_y)\n",
    "    return [ref_x, ref_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"3\"><b>3.0.6 Call geolocation to gather the aoi_coords:</b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aoi_coords = [geolocation(aoi.x1, aoi.y1, geotrans, latlon=False), geolocation(aoi.x2, aoi.y2, geotrans, latlon=False)]\n",
    "    print(f\"aoi_coords in EPSG {utm}: {aoi_coords}\")\n",
    "except TypeError:\n",
    "    print('TypeError')\n",
    "    display(Markdown(f'<text style=color:darkred>This error occurs if an AOI was not selected.</text>'))\n",
    "    display(Markdown(f'<text style=color:darkred>Note that the square tool icon in the AOI selector menu is <b>NOT</b> the selection tool. It is the zoom tool.</text>'))\n",
    "    display(Markdown(f'<text style=color:darkred>Read the tips above the AOI selector carefully.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='4'><b> 3.1 Subset (Crop) Data to Area of Interest </b> </font>\n",
    "<br>\n",
    "<font face='Calibri' size='3'>We now subset our data to our AOI. We must do this for both the interferograms and the coherence files. In this lab, we will also subset the amplitude image files for later display purposes, though this is not necessary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_paths = amp_full_paths + unw_full_paths + corr_full_paths\n",
    "print(\"Subsetting amplitude, coherence, and interferogram files.\")\n",
    "for tiff_path in tiff_paths:\n",
    "    path,tiff = os.path.split(tiff_path)\n",
    "\n",
    "    gdal_command = (f\"gdal_translate -epo -eco -projwin {aoi_coords[0][0]} \"\n",
    "                    f\"{aoi_coords[0][1]} {aoi_coords[1][0]} {aoi_coords[1][1]} \"\n",
    "                    f\"-projwin_srs 'EPSG:{utm}' -co \\\"COMPRESS=DEFLATE\\\" \"\n",
    "                    f\"-a_nodata 0 {tiff_path} {subset_folder}/{tiff} > /dev/null\")\n",
    "\n",
    "    # print(f\"\\nCalling the command: {gdal_command}\") \n",
    "    !{gdal_command} # Call the GDAL command. \n",
    "print(\"Subsetting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='3'><font face='Calibri'><font size='3'><b>NOTE:</b> It is possible GDAL returned an <b>error</b> for one or more of the interferogram, coherence, and amplitude files. This is because of our use of the \"-epo\" and \"-eco\" options. Those rasters which return an error are either partially or completely outside of our AOI. Including these empty files would cause errors in TRAIN and GIAnT. It is possible to include those files that only have partial extent within our AOI, but would require significantly more advanced processing which we exclude for simplicity. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>3.2 Perform some checks on the subset data</b></font>\n",
    "<br>  \n",
    "<font size='3'><b>3.2.0 Check that the Subsetted Tiffs have Pixels</b></font>\n",
    "<br>\n",
    "<font size='3'>Some of the subsetted geotiffs do not have pixels in our AOI despite the \"-epo\" and \"-eco\" options which should cause an error for all of these and skip them. Below, we will <b>check which geotiffs actually have pixels in our AOI and remove those that don't.</b> This can be done with a simple NaN search or by checking the band statistics of the file. </font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_subsets = f\"{subset_folder}/*.tif\"\n",
    "subset_paths = get_tiff_paths(paths_subsets)\n",
    "remove_nan_filled_tifs(subset_folder,subset_paths) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>3.2.1 Check the dimensions of the subset tiffs</b>\n",
    "<br>\n",
    "In some instances, the gdal_translate function may return subset imagery with slightly different extents. They may differ in size by a single pixel width in the x or y dimension. This is usually only a problem when multiple data sensors are used. Sensors will often have different pixel sizes and/or their pixel locations will be slightly offset from each other. Since all of our data comes from Sentinel-1, we probably don't need to worry but it is still good to double check.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_paths = get_tiff_paths(paths_subsets)\n",
    "subset_paths.sort()\n",
    "print(f\"Number of subset tifs: {len(subset_paths)}\")\n",
    "for path in subset_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'><b>3.2.2 Check that all subset images have the same dimensions</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixels_lines(geotiff_paths: list) -> list:\n",
    "    pixels = []\n",
    "    lines = []\n",
    "    for file in geotiff_paths:\n",
    "        img = gdal.Open(file)\n",
    "        pixels.append(img.RasterXSize)\n",
    "        lines.append(img.RasterYSize)\n",
    "    return {'pixels': set(pixels), 'lines': set(lines)}\n",
    "\n",
    "\n",
    "dimensions = get_pixels_lines(subset_paths)\n",
    "print(f\"Dimensions: {dimensions}\")\n",
    "if len(dimensions['pixels']) > 1 or len(dimensions['lines']) > 1:\n",
    "    print(\"Error: Not all subset images in stack have the same dimensions\")\n",
    "else:\n",
    "    print(\"Check successful. All subset images in stack have the same dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='5'> <b> 4. Create Input Files and Code for TRAIN </b> </font>\n",
    "<br>\n",
    "<font size='3'> In this section, we will use TRAIN to remove static atmospheric effects that can cause decoherence of the interferograms. This primarily corrects for effects caused by elevation differences between locations. <br>If we think of the propogating radar wave as a set of discrete rays, each ray will follow a different path. Those that reflect from elevated locations will pass through less of the atmosphere and therefore be less altered than those rays that reflect from locations at lower elevations. Without this correction, interferograms often produce exaggerated deformation. This is especially important in hazard monitoring of active volcanoes as a false alarm can be extremely costly and cause the general public to ignore future warnings. \n",
    "</font>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'>Let's create the input files and modify the files as required by TRAIN. The necessary items and actions are listed below. <br>\n",
    "        \n",
    "- parms_aps.txt\n",
    "    - List of parameters defining how TRAIN will run.\n",
    "- ifgday.txt\n",
    "    - Text file listing the primary and secondary date pairs. \n",
    "    - 2 column vector [primary secondary]\n",
    "    - Format: YYYYMMDD\n",
    "- Convert subsetted '.tif' Files to GCS Coordinates\n",
    "    - TRAIN requires the input '.tif' files to have a global coordinate system. \n",
    "    - We will convert them to EPSG:4326. \n",
    "- Adjust File Names\n",
    "    - Many SAR codes expect the input files to have a particular name format. \n",
    "    - For TRAIN, this is <font face='Courier New'>&lt;primary\\_date&gt;\\_&lt;secondary\\_date&gt;\\_&lt;unwrapped, amplitude, or coherence designation&gt;.tif</font>.\n",
    "<br></font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'> <b>4.0 Create parms_aps.txt file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>We will need to create the file 'parms_aps.txt'. TRAIN will read parameters from this file. We will first need to extract some information from the satellite metadata files. We'll start with getting the UTC time of the satellite pass over our study area. <br>\n",
    "        \n",
    "</font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "<font size='3'><b>4.0.0 Extract the UTC Time</b>\n",
    "<br>\n",
    "This information is contained in the file name.<br>\n",
    "Alternatively, it can be found in the metadata file named <font face='Courier New'>$<$primary timestamp$>$_$<$secondary timestamp$>$.txt</font> that comes with the interferogram.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getUTC_sat(files):\n",
    "    utc_hr = []\n",
    "    utc_min = []\n",
    "    utc_sec = []\n",
    "    for file in files:\n",
    "        vals = file.split('_')\n",
    "        tstamp = vals[0][9:16]\n",
    "        utc_hr.append(int(tstamp[0:2]))\n",
    "        utc_min.append(int(tstamp[2:4]))\n",
    "        utc_sec.append(int(tstamp[4:6]))    \n",
    "    \n",
    "    # UTC time as HH:MIN; we extract the median value and pad with up to 2 zeroes. \n",
    "    utc_sat = (f\"{str(int(np.median(utc_hr))).zfill(2)}:\"\n",
    "               f\"{str(int(np.median(utc_min))).zfill(2)}\")\n",
    "    # UTC time as an integer; Method from Tom Logan's prepGIAnT code\n",
    "    # Can also be found inside <date>_<date>.txt file and hard coded/extracted\n",
    "    c_l_utc = np.median(utc_hr)*3600 + np.median(utc_min)*60 + np.median(utc_sec) \n",
    "    return utc_sat, c_l_utc\n",
    "\n",
    "unw_files = [f for f in os.listdir(subset_folder) if f.endswith('_unw_phase.tif')] \n",
    "print(\"Unwrapped Phase Tifs:\")\n",
    "for file in unw_files: \n",
    "    print(file)\n",
    "    \n",
    "utc_sat, c_l_utc = getUTC_sat(unw_files)\n",
    "print(f\"\\nUTC in int form: {c_l_utc}\")\n",
    "print(f\"UTC in human-readable form: {utc_sat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='3'><b>4.0.1 Create parms_aps.txt file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>Make the parms_aps.txt file. This gives TRAIN information on how to process the data. </font>\n",
    "<br><br>\n",
    "<font color='green'>Note: Ignore the deprecation warnings generated by this code cell. This is happening under-the-hood in pyproj's geopandas calls and will go away with future updates. It is a known issue, not something we can control, and will not impact results.</font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_proj = pyproj.Proj(f'EPSG:{utm}')\n",
    "out_proj = pyproj.Proj('EPSG:4326')\n",
    "llx, lly = pyproj.transform(in_proj,out_proj,coords[0][0],coords[0][1])\n",
    "urx, ury = pyproj.transform(in_proj,out_proj,coords[1][0],coords[1][1])\n",
    "region_lats = abs(np.diff([lly,ury])[0]) + extra\n",
    "region_lons = abs(np.diff([llx,urx])[0]) + extra\n",
    "\n",
    "!mkdir -p {merra2_datapath} # create the directory\n",
    "\n",
    "parms_aps_Template = '''\n",
    "# Input parameters for TRAIN\n",
    "\n",
    "crop_flag: n\n",
    "date_origin: file\n",
    "dem_null: -32768\n",
    "DEM_origin: asf\n",
    "DEM_file: {7}\n",
    "era_data_type: {1}\n",
    "ifgday_file: {2}\n",
    "incidence_angle: {9}\n",
    "lambda: {8}\n",
    "look_angle: 21\n",
    "meric_perc_coverage: 80\n",
    "merra2_datapath: {4}\n",
    "non_defo_flag: n\n",
    "region_lat_range: {5}\n",
    "region_lon_range: {6}\n",
    "region_res: 0.008333000000000\n",
    "save_folder_name: aps_estimation\n",
    "small_baseline_flag: n\n",
    "stamps_processed: n\n",
    "UTC_sat: {3}\n",
    "\n",
    "'''\n",
    "\n",
    "with open(os.path.join(train_dir,'parms_aps.txt'), 'w') as fid:\n",
    "    fid.write(parms_aps_Template.format(train_dir, era_data_type,\n",
    "                                        ifgday_file, utc_sat,\n",
    "                                        merra2_datapath, region_lats,\n",
    "                                        region_lons, dem_path,\n",
    "                                        lambda_meters, incidence_angle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>You may notice that the <font face='Courier New'>parms_aps.txt</font> file we created has very little geographic information; we've only included the width and height of the subsetted interferograms in decimal degrees (the variables <font face='Courier New'>region_lats</font> and <font face='Courier New'>region_lons</font>). This is because we will use one of our subsetted and converted tiffs as a geographic reference file. Otherwise, we would need to create a text file that contains the latitude and longitude of each pixel in the interferogram.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.1 Create ifgday.mat file</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>This gives TRAIN the primary and secondary dates of the interferograms as 2 column vectors. The dates must be separated by a single space.<br>\n",
    "        \n",
    "- Interferogram dates stored as a matrix with name ifgday and size [n_ifgs 2]. \n",
    "- Primary image is in the first and secondary is in the second column. \n",
    "- Specify dates as a numeric value in YYYYMMDD format.\n",
    "\n",
    "    </font>\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the primary and secondary dates. \n",
    "primary_dates = []\n",
    "secondary_dates = []\n",
    "for file in unw_files:\n",
    "    tstamps = file.split('_')\n",
    "    primary_dates.append(tstamps[0][0:8])\n",
    "    secondary_dates.append(tstamps[1][0:8])\n",
    "# Sort the dates according to the primary dates. \n",
    "p_dates, s_dates = (list(t) for t in zip(*sorted(zip(primary_dates, secondary_dates))))\n",
    "\n",
    "# write values to the 'ifgday_file'; make sure there is only 1 space between the dates.\n",
    "with open( os.path.join(train_dir, ifgday_file), 'w') as ifg:\n",
    "    for i, date in enumerate(p_dates):\n",
    "        primary_date = date # pull out primary date\n",
    "        secondary_date = s_dates[i] # pull out secondary date\n",
    "        ifg.write(f'{primary_date} {secondary_date}\\n') \n",
    "\n",
    "# print the contents of the file we just wrote\n",
    "with open(os.path.join(train_dir,ifgday_file),'r') as ifg:\n",
    "    ifg_contents = ifg.read()\n",
    "print(ifg_contents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.2 Convert Subsetted '.tif' Files to GCS Coordinates</b> </font>\n",
    "    <br>\n",
    "    <font size='3'>The ASF version of TRAIN requries our subsets to be in GCS coordinates (i.e., for every pixel in the subset to be designated by latitude and longitude). Currently, they are in a UTM coordinate system, which gives pixel location in meters based on a local coordinate system. We now will reproject our subsets to GCS.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.2.0 Define the projection to which we will be reprojecting the subsets. This is typically designated as EPSG:4326.</b></font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_TRAIN = '4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.2.1 Create the converted subsets.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in unw_files:\n",
    "    in_file = os.path.join(subset_folder, file)\n",
    "    out_file = os.path.join(corrected_folder, file)\n",
    "    cmd = f\"gdalwarp -t_srs EPSG:{coord_TRAIN} {in_file} {out_file}\"\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'>\n",
    "    <font size='4'><b>4.3 Adjust File Names</b></font>\n",
    "    <font size='3'><br>TRAIN expects the file naming format, <font face='Courier New'>&lt;primary_date&gt;_&lt;secondary_date&gt;_&lt;unwrapped, amplitude, or coherence designation&gt;.tif</font>. We will adjust the files to match this name convention. The code below assumes that the files all come from Sentinel-1 and that every interferogram has a unique primary and secondary date pair. <br><b>This is not always true; some interferograms will have identical primary/secondary date pairs, but have been taken at different times.</b> This is a relatively rare occurrence, but it is good to keep in mind. To keep this exercise relatively simple, we assume each interferogram has a unique primary/secondary date pair.</font>\n",
    "<br><br>\n",
    "<b>4.3.0 Write a function to rename the files</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files_for_train(data_dir, files): \n",
    "    for file in files:\n",
    "        if \"T\" in file: # only change files needing to be renamed (containing a 'T') \n",
    "            old_name, old_ext = os.path.splitext(file)\n",
    "            t_stamps = old_name.split('_')\n",
    "            primary = t_stamps[0][0:8] \n",
    "            secondary = t_stamps[1][0:8]\n",
    "            if \"_unw\" in file:\n",
    "                new_name = f\"{primary}_{secondary}_unw_phase{old_ext}\"\n",
    "            elif \"_corr\" in file:\n",
    "                new_name = f\"{primary}_{secondary}_corr{old_ext}\"\n",
    "            elif \"_amp\" in file:\n",
    "                new_name = f\"{primary}_{secondary}_amp{old_ext}\"\n",
    "            \n",
    "            exists = os.path.isfile(os.path.join(data_dir, new_name))\n",
    "            if exists:\n",
    "                print(f\"File already exists: {new_name}\")\n",
    "            else:\n",
    "                os.rename(os.path.join(data_dir, file),\n",
    "                          os.path.join(data_dir, new_name))\n",
    "    print(\"Files renamed.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.3.1 Rename the subset files and save a list of the new filenames.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ext = '.tif'\n",
    "files = [f for f in os.listdir(subset_folder) if f.endswith(file_ext)] \n",
    "files.sort()\n",
    "\n",
    "rename_files_for_train(subset_folder, files)\n",
    "files_subset = [f for f in os.listdir(subset_folder)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>4.3.2 Rename the converted files and save a list of the new filenames.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(corrected_folder) if f.endswith(file_ext)] \n",
    "files.sort()\n",
    "\n",
    "rename_files_for_train(corrected_folder, files)\n",
    "files_converted = [f for f in os.listdir(corrected_folder)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='5'><b>5. Run TRAIN</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have now created all of the necessary files to run TRAIN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.0 Minor Set Up</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We have to set up some path information in order to run TRAIN. <i>Eventually, this will be modified so we don't have to include the full path to TRAIN.</i> Additionally, we show multiple ways in which to call TRAIN.</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/usr/local/TRAIN/src\"\n",
    "aps_weather_model_path = os.path.join(train_path,'aps_weather_model.py')\n",
    "print(f\"aps_weather_model.py path: {aps_weather_model_path}\")\n",
    "georef_path = os.path.join(home, corrected_folder, files_converted[0])\n",
    "print(f\"\\nPath to a georeference file: {georef_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some help information\n",
    "!python $train_path/aps_weather_model.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.1 TRAIN Steps 0-3</b></font>\n",
    "    <br>\n",
    "    <font size='3'>Now we run steps 0 through 3. Step 4 requires a few extra actions. \n",
    "    <br><br>\n",
    "        <b>Step 0: Identify weather data files to download.</b></font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "!python $train_path/aps_weather_model.py -g {georef_path} 0 0\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 1: Download weather data.</b>\n",
    "<br>\n",
    "This will download a series of '*.nc4' files for each primary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete any existing MERRA2 downloads to prevent old files from being used. \n",
    "try:\n",
    "    shutil.rmtree(merra2_datapath)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "!python $train_path/aps_weather_model.py -g {georef_path} 1 1\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 2: Calculate wet and hydrostatic zenith delays</b>\n",
    "<br>\n",
    "This will download a set of '*.xyz' files and use those \n",
    "to calculate the necessary delays.\n",
    "<br><br>\n",
    "Note: This step will error if the subset area being processed is too small. Unfortunately, the minimum area size does not appear to be documented. If you run into this issue, a little trial and error with subset sizes may be required. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "!python $train_path/aps_weather_model.py -g {georef_path} 2 2\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Step 3 - Calculate the SAR delays</b>\n",
    "<br>\n",
    "This produces *_*_{hydro_correction, wet_correction, and correction}.bin files\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "!python $train_path/aps_weather_model.py -g {georef_path} 3 3\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>Step 3 may give a strange message: \"No correction for &lt;insert date&gt;\". If this occurs, it is most likely because TRAIN wasn't able to access the MERRA2 files due to missing permissions in your Earthdata user account. This can be done by going to your <a href=\"https://urs.earthdata.nasa.gov/profile\" target=\"_blank\">EarthData's profile page</a>, clicking <b>Applications</b> and selecting <b>Approved Applications</b> from the drop down menu. Select <b>Approve More Applications</b> at the bottom left, search for <b>NASA GESDISC DATA ARCHIVE</b>, select it, and agree to the terms and conditions. Once that is complete, restart the kernel in the kernel menu and rerun the notebook.</font></font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.2 TRAIN Step 4</b></font>\n",
    "<br>\n",
    "<font size='3'>In step 4, we apply the correction to our <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase.tif&gt;</font> files. \n",
    "<br><br>\n",
    "For this step, we need to do 2 things first:\n",
    "<ol>\n",
    "<li>Move all of the '*.bin' files into the same directory as our converted geotiffs.</li>\n",
    "<li>Move into the converted geotiffs directory.</li>\n",
    "</ol>\n",
    "<b>5.2.0 Create a list of our .bin files</b></font>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ext = '.bin'\n",
    "files = [f for f in os.listdir('.') if f.endswith(file_ext)] \n",
    "files.sort()\n",
    "print(len(files))\n",
    "\n",
    "print(*files, sep=\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.2.1 Move the .bin files into the converted geotiffs directory and then move into that directory</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    try:\n",
    "        shutil.move(file,os.path.join(corrected_folder,file))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError, {file} does not exist.\")\n",
    "        print(\"Have you already moved your .bin files?\")\n",
    "        break\n",
    "\n",
    "try:        \n",
    "    os.chdir(f\"{home}/{corrected_folder}\")\n",
    "except FileNotFoundError:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.2.2 Step 4 - Subtract calculated delay from interferograms</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netrc_path = '/home/jovyan/.netrc'\n",
    "with open(netrc_path, 'w+') as netrc:\n",
    "    netrc.write(f'machine urs.earthdata.nasa.gov login {login.username} password {login.password}')\n",
    "\n",
    "!python $train_path/aps_weather_model.py -g {georef_path} 4 4\n",
    "\n",
    "os.remove(netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.2.3 Return to the home directory for this analysis</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(home)\n",
    "except FileNotFoundError:\n",
    "    raise\n",
    "    \n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>If we check our <font face='Courier New'>corrected_folder</font> directory, we will find new files with the naming convention <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase\\_corrected.tif&gt;</font>. The uncorrected files, <font face='Courier New'>&lt;\\*\\_\\*\\_unw\\_phase.tif&gt;</font>, are now technically superfluous and can be deleted. However, we will keep these files for the purpose of comparing the corrected and uncorrected times series in Part 2.\n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.3 Comparison of Corrected and Uncorrected Unwrapped Phase</b></font>\n",
    "    <br>\n",
    "    <font size='3'>We will make a quick and simple comparison between the corrected and uncorrected unwrapped phase geotiffs. This is meant to highlight the importance of these atmospheric corrections.\n",
    "<br><br>\n",
    "    <b>5.3.0 Gather the paths to the corrected and uncorrected unwrapped phase tifs</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_cor = f\"{home}/{corrected_folder}/*_unw_phase_corrected.tif\"\n",
    "paths_unc = f\"{home}/{corrected_folder}/*_unw_phase.tif\"\n",
    "cor_paths = get_tiff_paths(paths_cor)\n",
    "unc_paths = get_tiff_paths(paths_unc)\n",
    "\n",
    "# Uncomment to view paths\n",
    "# print_tiff_paths(cor_paths)\n",
    "# print_tiff_paths(unc_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.3.1 Plot a corrected and uncorrected unwrapped phase tif for comparison</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "corrected = gdal.Open(cor_paths[0])\n",
    "uncorrected = gdal.Open(unc_paths[0])\n",
    "im_c = corrected.GetRasterBand(1).ReadAsArray()\n",
    "im_u = uncorrected.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax1.imshow(im_c, cmap='gray')\n",
    "ax2.imshow(im_u, cmap='gray')\n",
    "plt.title('Corrected and Uncorrected Unwrapped Phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.3.2 Plot a difference map of the two images</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "difference = np.subtract(im_c, im_u)\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "fig_plot = ax1.imshow(difference, cmap='RdBu')\n",
    "cbar = fig.colorbar(fig_plot, fraction=0.24, pad=0.02)\n",
    "cbar.ax.set_xlabel('mm')\n",
    "cbar.ax.set_ylabel('Correction Difference', rotation=270, labelpad=20)\n",
    "ax1.set(title='TRAIN Correction Difference Map [mm]')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>In looking at the Correction Difference Map, we can see that size of the correction correlates with surface elevation. Without this correction, a volcanologist would see the extra elevation difference as an indication of magma injection and possible eruptive activity.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.4 Convert back to original coordinate system</b></font>\n",
    "<br>\n",
    "<font size='3'>GIAnT requires the interferograms to be in a particular coordinate system. The original coordinate system is one of those accepted, so we will convert our interferograms back to that.\n",
    "<br><br>\n",
    "<b>5.4.0 Display our EPSGs</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"original coordiante system = EPSG:{utm}\")\n",
    "print(f\"TRAIN coordinate system =    EPSG:{coord_TRAIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.1 Grab the paths to all the corrected tiffs</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = f\"{corrected_folder}/*.tif\"\n",
    "tiff_paths = get_tiff_paths(paths)\n",
    "\n",
    "# Uncomment to view paths\n",
    "print_tiff_paths(tiff_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.2 check that the current coordinate system of the files is different from the desired.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "print(f\"Current UTM Types & Zones = EPSG:{list(set(utm_zones))}\")\n",
    "print(f\"Expected current system   = EPSG:{coord_TRAIN}\")\n",
    "print(f\"Desired coordinate system = EPSG:{utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.3 Reproject the corrected tiffs.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in tiff_paths:\n",
    "    # Designate the output file and its path; ideally these are the same. \n",
    "    # GDAL can't do that (it'll overwrite data sometimes), so for each  \n",
    "    # we create new file with the correct projection, delete the old file, \n",
    "    # and then rename the newly created file. \n",
    "    base_path ,filename = os.path.split(file)\n",
    "    desig = 'TEMP_'\n",
    "    outfile = os.path.join(base_path, f\"{desig}{filename}\")\n",
    "    # create the convert command\n",
    "    cmd = f\"gdalwarp -t_srs EPSG:{utm} {file} {outfile}\"\n",
    "    !{cmd} # convert the file\n",
    "    \n",
    "    # delete the file in the EPSG:4326 coordinate system\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # rename the file in the utm coordinate system to our original name of 'inFile'. \n",
    "    try:\n",
    "        os.rename(outfile, file)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.4.4 Confirm that the files are now in the correct projection</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utm_zones, utm_types = get_utm_zones_types(tiff_paths)\n",
    "print(f\"Current UTM Types & Zones = EPSG:{list(set(utm_zones))}\")\n",
    "print(f\"Expected current system   = EPSG:{utm}\")\n",
    "print(f\"Desired coordinate system = EPSG:{utm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='4'><b>5.5 Take care of some final details</b></font>\n",
    "    <br><br>\n",
    "<font size='3'><b>5.5.0 Do another pixel check</b></font>\n",
    "<br>\n",
    "<font size='3'>Check the pixel sizes again, and then do the pixel correction if necessary.\n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels_lines = get_pixels_lines(tiff_paths)\n",
    "print(pixels_lines)\n",
    "if len(pixels_lines['pixels']) == 1 and \\\n",
    "len(pixels_lines['lines']) == 1:\n",
    "    print(\"All images in the stack have the same dimensions\")\n",
    "else:\n",
    "    print(\"Error: Images with different dimensions exist in the stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>5.5.1 Pickle the information we need to access in the Part 2 notebook</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_pickle)\n",
    "filename = 'part1_pickle'\n",
    "outfile = open(filename,'wb')\n",
    "\n",
    "pickle.dump(to_pickle, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri'><font size='3'>\n",
    "You have now corrected the interferograms for atmospheric conditions and can proceed to Part 2: GIAnT. \n",
    "</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face='Calibri' size='3'><b>Print the path to the analysis directory, which you can copy/paste into Part 2</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>GEOS 657 Microwave Remote Sensing - Version 2.1.2 - September 2020</i>\n",
    "    <br>\n",
    "        <b>Version Changes:</b>\n",
    "    <ul>\n",
    "    <li>Correct reprojection code</li>\n",
    "    </ul>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
